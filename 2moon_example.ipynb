{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BbYf-tARKDBL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau,StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYmmbIZ2LQSB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "op3RuvP3KDBQ"
   },
   "outputs": [],
   "source": [
    "# Colors from Colorbrewer Paired_12\n",
    "colors = [[31, 120, 180], [51, 160, 44]]\n",
    "colors = [(r / 255, g / 255, b / 255) for (r, g, b) in colors]\n",
    "\n",
    "def plot_losses(train_history, val_history):\n",
    "    x = np.arange(1, len(train_history) + 1)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(x, train_history, color=colors[0], label=\"Training loss\", linewidth=2)\n",
    "    plt.plot(x, val_history, color=colors[1], label=\"Validation loss\", linewidth=2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(\"Evolution of the training and validation loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhL-RD5eKDBR"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# hyper parameters used in toy example\n",
    "nb_proto=10\n",
    "nb_layer=20\n",
    "n_samples=2000\n",
    "dist='cos'\n",
    "loss_uncer_param=1\n",
    "loss_proto_param=0.1\n",
    "loss_entropy_param=20\n",
    "num_classes = 2\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "nb_PC=2 # nb components of PCA\n",
    "\n",
    "Softmax = nn.Softmax(dim=1)\n",
    "\n",
    "# creating the dataset\n",
    "noise = 0.1\n",
    "X_train, y_train = sklearn.datasets.make_moons(n_samples=n_samples, noise=noise)\n",
    "X_test, y_test = sklearn.datasets.make_moons(n_samples=200, noise=noise)\n",
    "ds_train = torch.utils.data.TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train))\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "ds_test = torch.utils.data.TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test))\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size=200, shuffle=False)\n",
    "\n",
    "# creating the ood data (won't be used in any training, will be used just in illustrations)\n",
    "ood_x, _ = sklearn.datasets.make_circles(n_samples=200,shuffle=True, noise=0.05, random_state=42, factor=0.99)\n",
    "ood_x = ood_x * 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-BL2zK1KDBS"
   },
   "outputs": [],
   "source": [
    "# We build two MLPs, one with DM and one without\n",
    "def centered_cov_torch(x):\n",
    "    n = x.shape[0]\n",
    "    res = 1 /n * x.t().mm(x)\n",
    "    #res = x.t().mm(x)\n",
    "    return res\n",
    "\n",
    "def normpdf(x, gaussian):\n",
    "    n = x.shape[0]\n",
    "    var = 0.1#float(sd)**2\n",
    "\n",
    "    out=[]\n",
    "    for i in range(n):\n",
    "        num =np.exp(-0.5*np.matmul(np.matmul((x[i]-gaussian['mean']), gaussian['cov_inv']),np.transpose((x[i]-gaussian['mean']))))/gaussian['denom']\n",
    "        #print(np.shape(num),num)\n",
    "        out.append(num)\n",
    "    out=np.array(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "class Distanceminimi_Layer_learned(nn.Module):\n",
    "    def __init__(self, in_features=0, out_features=0, dist='lin'):\n",
    "        super(Distanceminimi_Layer_learned, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dist=dist\n",
    "        self.omega = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "\n",
    "        nn.init.normal_(self.omega, mean=0, std=1)#/self.out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        prots = self.omega.unsqueeze(0)\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        if self.dist == 'l2':\n",
    "            x = -torch.pow(x - prots, 2).sum(-1)  # shape [n_query, n_way]\n",
    "        elif self.dist == 'cos':\n",
    "            x = F.cosine_similarity(x, prots, dim=-1, eps=1e-30)\n",
    "        elif self.dist == 'lin':\n",
    "            x = torch.einsum('izd,zjd->ij', x, prots)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# MLP with DM\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, features, nb_prototype,dist='cos'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(2, features)\n",
    "        self.fc2 = nn.Linear(features, features)\n",
    "        self.fc3 = nn.Linear(features, features)\n",
    "        self.dist =dist\n",
    "        self.DMlayer = Distanceminimi_Layer_learned(in_features = features, out_features = nb_prototype, dist=dist)\n",
    "        self.DMBN = nn.BatchNorm1d(nb_prototype)\n",
    "        self.linear1  = nn.Linear(nb_prototype, features)\n",
    "        self.linear2  = nn.Linear(features, 1)\n",
    "        \n",
    "        self.fc4 = nn.Linear(nb_prototype, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x1 = self.DMlayer(x)\n",
    "        if self.dist=='cos' :x2 = torch.exp(-x1)\n",
    "        else :x2 = torch.exp(x1)\n",
    "        x2_bn = self.DMBN(x2)\n",
    "        uncer = F.relu(self.linear1(x2_bn))\n",
    "        uncer = self.linear2(uncer).squeeze()\n",
    "        pred = self.fc4(x2_bn)\n",
    "        \n",
    "        return pred, uncer, x, x1, x2\n",
    "\n",
    "    def forward_conf(self, x):\n",
    "        x1 = self.DMlayer(x)\n",
    "        if self.dist == 'cos':\n",
    "            x2 = torch.exp(-x1)\n",
    "        else:\n",
    "            x2 = torch.exp(x1)\n",
    "        x2 = self.DMBN(x2)\n",
    "        x2 = F.relu(self.linear1(x2))\n",
    "        uncer = self.linear2(x2).squeeze()\n",
    "        return uncer\n",
    "\n",
    "    def loss_kmeans(self):\n",
    "        param = self.DMlayer.omega\n",
    "        loss = torch.mean(torch.cdist(param, param))\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnWK6O45KDBT"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch,log_interval=20):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total=0\n",
    "    for batch_idx, data in enumerate(train_loader, 0):\n",
    "\n",
    "        x, y  = data\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        y_pred, uncer, embedding, embeddingDM, _ = model(x)\n",
    "        loss_pred =  F.cross_entropy(y_pred, y, reduction='none')\n",
    "        loss_detach = loss_pred.detach()\n",
    "        loss_detach = loss_detach - loss_detach.min()\n",
    "        loss_detach = loss_detach/loss_detach.max()\n",
    "        loss_proto = model.loss_kmeans()\n",
    "        loss_uncer = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([0.01]).cuda())(uncer, loss_detach)\n",
    "\n",
    "        embeddings_proba = Softmax(embeddingDM)\n",
    "        embeddings_entropy = torch.sum(embeddings_proba * torch.log(embeddings_proba), dim=1)\n",
    "        loss_entropy = torch.mean(embeddings_entropy)\n",
    "        \n",
    "        loss = loss_pred.mean() + loss_uncer*loss_uncer_param - loss_proto_param*loss_proto + loss_entropy_param*loss_entropy\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        y_pred = torch.max(y_pred, dim=1)[1]\n",
    "        correct += y_pred.eq(y.data).cpu().sum()\n",
    "        total += y.size(0)\n",
    "        acc = 100.*correct/total\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tACCU: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(x), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item(), acc.item() ))\n",
    "\n",
    "    return acc.item()\n",
    "\n",
    "def test(model, test_loader, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(test_loader, 0):\n",
    "            x, y  = data\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            y_pred, _, _, _, _ = model(x)\n",
    "            y_pred = Softmax(y_pred)\n",
    "            y_pred = torch.max(y_pred, dim=1)[1]\n",
    "\n",
    "            correct += y_pred.eq(y.data).cpu().sum()\n",
    "            total += y.size(0)\n",
    "        acc = 100.*correct/total\n",
    "        print('Test Epoch: {} \\t ACCU: {:.6f}'.format(epoch, acc))\n",
    "    return acc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfhb4LgjZVkd"
   },
   "source": [
    "#### Training DNN with DM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qMfw6xQ4KDBV",
    "outputId": "f13ec2d9-bb63-4c47-c2f2-aec5e5eb8f7b"
   },
   "outputs": [],
   "source": [
    "# we perfom a training of the MODEL with DM\n",
    "num_epochs = 17\n",
    "model = Model(nb_layer, nb_proto,dist)\n",
    "model = model.cuda()\n",
    "train_history = []\n",
    "val_history = []\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "for epoch in range(num_epochs):\n",
    "    acc = train(model, dl_train, optimizer, epoch)\n",
    "    train_history.append(acc)\n",
    "    acc=test(model, dl_test, epoch)\n",
    "    val_history.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "oovhsU50KDBX",
    "outputId": "44588772-3332-48ee-d7fd-02b349a4d99a"
   },
   "outputs": [],
   "source": [
    "# we perfom an evaluation of the MODEL with DM\n",
    "\n",
    "# we build the test set\n",
    "domain = 5\n",
    "x = np.linspace(-domain+0.5, domain+0.5, 100)\n",
    "y = np.linspace(-domain, domain, 100)\n",
    "\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "X_grid = np.column_stack([xx.flatten(), yy.flatten()])\n",
    "X = np.column_stack([xx.flatten(), yy.flatten()])\n",
    "\n",
    "X_vis, y_vis = sklearn.datasets.make_moons(n_samples=500, noise=noise)\n",
    "mask = y_vis.astype(np.bool)\n",
    "\n",
    "model.eval()\n",
    "# we evaluate on the test set\n",
    "with torch.no_grad():\n",
    "    \n",
    "    output, confidence, embedding, embeddingDM, _ = model(torch.from_numpy(X_grid).float().cuda())\n",
    "    output = torch.max(output, dim=1)\n",
    "    confidence = 1 - nn.Sigmoid()(confidence)\n",
    "\n",
    "confidence = confidence.reshape(xx.shape).cpu()\n",
    "output = output[1].reshape(xx.shape).cpu()\n",
    "# we visualize\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.contourf(x, y, confidence, cmap='cividis')\n",
    "plt.scatter(X_vis[mask,0], X_vis[mask,1])\n",
    "plt.scatter(X_vis[~mask,0], X_vis[~mask,1])\n",
    "plt.scatter(ood_x[:,0], ood_x[:,1])\n",
    "plt.title('uncertainty map')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.contourf(x, y, output, cmap='cividis')\n",
    "plt.scatter(X_vis[mask, 0], X_vis[mask, 1])\n",
    "plt.scatter(X_vis[~mask, 0], X_vis[~mask, 1])\n",
    "plt.scatter(ood_x[:,0], ood_x[:,1])\n",
    "plt.title('decision map')\n",
    "\n",
    "name = 'results_uncer_BEFORE_training_gaussian.pdf'\n",
    "fig.savefig(name, dpi=200)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q0Ns5gVCazjL",
    "outputId": "873586ae-b8be-490f-d147-3004c1cf68c7"
   },
   "outputs": [],
   "source": [
    "# we save the model\n",
    "modelsave = Model(nb_layer, nb_proto,dist)\n",
    "modelsave.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VYo0ZXyawKn"
   },
   "source": [
    "#### Optional: Fine-tuning g_unc in DNN with DM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "id": "pnucQNKOKDBa",
    "outputId": "fb75e262-2282-4075-fa79-50bffa7c5270"
   },
   "outputs": [],
   "source": [
    "# we build OOD data to train the g^{unce} for OOD\n",
    "# Our OOD process generation is inspired by https://arxiv.org/abs/2202.01197\n",
    "# we concatenate the latent space of the training dataset\n",
    "\n",
    "with torch.no_grad():\n",
    "    output, confidence, z_train, embeddingDM,_ = model(torch.from_numpy(X_train).float().cuda())\n",
    "\n",
    "prototypes = model.DMlayer.omega\n",
    "\n",
    "\n",
    "z_embed = torch.from_numpy(X_train) # z_embed is the embedding of the DNN\n",
    "\n",
    "#  we apply a PCA on this space to have a better representation\n",
    "pca = PCA(n_components=nb_PC).fit(prototypes.detach().cpu().numpy())\n",
    "prototypes_embed=pca.fit_transform(prototypes.detach().cpu().numpy())\n",
    "prototypes_embed=torch.from_numpy(prototypes_embed)\n",
    "\n",
    "\n",
    "mean_data = torch.mean(z_embed, dim=0)\n",
    "mean_data_numpy = mean_data.cpu().numpy().astype(np.float32)\n",
    "covariance =centered_cov_torch(z_embed-mean_data).cpu().numpy().astype(np.float32)\n",
    "covariance += 0.01 * np.eye(nb_PC, dtype=float)\n",
    "covariance_INV = np.linalg.inv(covariance)\n",
    "denom = np.linalg.det((2 * math.pi * covariance)) ** 0.5\n",
    "\n",
    "#  we build a gaussian ditribution representing the dataset\n",
    "gaussian={\"mean\":mean_data_numpy,\"cov\":covariance,\n",
    "          \"cov_inv\":covariance_INV,\"denom\":denom}\n",
    "#  we sample data according to the gaussian distribution\n",
    "x = np.random.multivariate_normal(gaussian['mean'], gaussian['cov'], (n_samples*2000))\n",
    "proba = normpdf(x, gaussian)\n",
    "#   we take the less probable data which represent OOD\n",
    "x_sample = x[proba < 1.0e-2]\n",
    "x_sample_OOD=x_sample[0:n_samples+200]\n",
    "x_sample_OOD_train=x_sample_OOD[0:n_samples]\n",
    "x_sample_OOD_val=x_sample_OOD[n_samples:]\n",
    "\n",
    "z_embed = torch.cat((prototypes_embed, z_embed), 0)\n",
    "z_embed2 = z_embed.clone().numpy()\n",
    "#   we  concatenate everything for visualization\n",
    "z_embed=np.concatenate((z_embed2[0:nb_proto],np.expand_dims(mean_data, axis=0),x_sample_OOD,z_embed2[nb_proto:]),axis=0)\n",
    "\n",
    "#   we visualise the results\n",
    "class_proto=np.zeros(nb_proto+1)\n",
    "class_proto[nb_proto]=1\n",
    "class_OOD=2*np.ones(len(x_sample_OOD))\n",
    "y_train2 = np.concatenate((class_proto,class_OOD, y_train+3), axis=0)\n",
    "\n",
    "classes = ['proto','mean','OOD','blue', 'orange']\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "plots = []\n",
    "markers = ['o', ',', 'x', '+', 'v', '^', '<', '>', 's', 'd']\n",
    "for i, c in enumerate(classes):\n",
    "    ind = (y_train2 == i).tolist()\n",
    "    color = cm.jet([i / len(classes)] * sum(ind))\n",
    "    if i==0:\n",
    "        #color = cm.jet([i / len(classes)] * sum(ind))\n",
    "        plots.append(plt.scatter(z_embed[ind, 0], z_embed[ind, 1], marker=markers[i], c=color, s=18, label=i))\n",
    "    else :\n",
    "        plots.append(plt.scatter(z_embed[ind, 0][0:50], z_embed[ind, 1][0:50], marker=markers[i], c=color[0:50], s=18, label=i))\n",
    "\n",
    "plt.axis('off')\n",
    "plt.legend(plots, classes, fontsize=14, loc='upper right')\n",
    "plt.title('InD data - OOD data - Prototypes', fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWca6-mJKDBa"
   },
   "outputs": [],
   "source": [
    "#   we build a new trainig set.\n",
    "x_sample_OOD_train_1 = torch.from_numpy(x_sample_OOD_train).float()\n",
    "X_train_ind = torch.from_numpy(X_train).float()\n",
    "X_train0 = torch.cat((X_train_ind, x_sample_OOD_train_1))\n",
    "y_train1 = torch.zeros_like(X_train_ind[:,0])\n",
    "y_train2 = torch.ones_like(x_sample_OOD_train_1[:,0])\n",
    "y_train0 = torch.cat((y_train1, y_train2))\n",
    "\n",
    "x_sample_OOD_val_1 = torch.from_numpy(x_sample_OOD_val).float()\n",
    "X_test_ind = torch.from_numpy(X_test).float()\n",
    "X_test0 = torch.cat((X_test_ind, x_sample_OOD_val_1))\n",
    "y_test1 = torch.zeros_like(X_test_ind[:,0])\n",
    "y_test2 = torch.ones_like(x_sample_OOD_val_1[:,0])\n",
    "y_test0 = torch.cat((y_test1, y_test2))\n",
    "\n",
    "ds_train = torch.utils.data.TensorDataset(X_train0, y_train0)\n",
    "dl_train_ood = torch.utils.data.DataLoader(ds_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "ds_test = torch.utils.data.TensorDataset(X_test0, y_test0)\n",
    "dl_test_ood = torch.utils.data.DataLoader(ds_test, batch_size=200, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Di0MIfqKDBb"
   },
   "outputs": [],
   "source": [
    "criterionBCE = nn.BCEWithLogitsLoss() \n",
    "def train( model, train_loader, optimizer, epoch,log_interval=20):\n",
    "    model.train()\n",
    "    loss_cpu=0\n",
    "    correct = 0\n",
    "    total=0\n",
    "    for batch_idx, data in enumerate(train_loader, 0):\n",
    "\n",
    "        x, y  = data\n",
    "        x, y = x.cuda(), y.cuda()\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        _, uncer, _, _ , _ = model(x)\n",
    "        loss = criterionBCE(uncer, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_cpu+=loss.cpu().item()\n",
    "        uncer_pred = torch.sigmoid(uncer).detach()\n",
    "        uncer_pred = (uncer_pred>0.5).long()\n",
    "        correct += uncer_pred.eq(y.data).cpu().sum()\n",
    "        total += y.size(0)\n",
    "        acc = 100.*correct/total\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} \\tACCU: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(x), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item(), acc.item() ))\n",
    "\n",
    "    return acc.item()\n",
    "\n",
    "def test(model, test_loader,epoch):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss_MSE =0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(test_loader, 0):\n",
    "            x, y  = data\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            _, uncer_pred, _, _ , _ = model(x)\n",
    "            uncer_pred = nn.Sigmoid()(uncer_pred)\n",
    "            uncer_pred = (uncer_pred>0.5).long()\n",
    "\n",
    "            correct += uncer_pred.eq(y.data).cpu().sum()\n",
    "            total += y.size(0)\n",
    "        # Save checkpoint when best model\n",
    "        acc = 100.*correct/total\n",
    "        print('Test Epoch: {} \\t ACCU: {:.6f}'.format(epoch, acc))\n",
    "    return acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nL4Le3mBKDBc",
    "outputId": "16595cfe-b1c5-4933-80ce-e735e47ba9ff"
   },
   "outputs": [],
   "source": [
    "#   we train g^{unce} to detect  OOD data\n",
    "num_epochs=35\n",
    "model=model.cuda()\n",
    "train_history = []\n",
    "val_history = []\n",
    "model.load_state_dict(modelsave.state_dict())\n",
    "# just fine tune the g_unc\n",
    "param = list(model.linear1.parameters()) + list(model.linear2.parameters())\n",
    "optimizer = torch.optim.Adam(param, lr=0.001,  weight_decay=1e-3)\n",
    "lr_scheduler =  StepLR(optimizer, step_size=14, gamma=10)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "\n",
    "    acc = train(model, dl_train_ood, optimizer, epoch)\n",
    "    train_history.append(acc)\n",
    "    acc=test(model, dl_test_ood,epoch)\n",
    "    lr_scheduler.step()\n",
    "    val_history.append(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "zkbJqil6KDBc",
    "outputId": "0a403fd5-712d-4656-9f36-b3934f3436d2"
   },
   "outputs": [],
   "source": [
    "#   we  vizualise the training curb\n",
    "plot_losses(train_history, val_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "NOMxKOXWKDBd",
    "outputId": "2508466e-5327-4af0-e773-09c6eb0e7014"
   },
   "outputs": [],
   "source": [
    "#   we  plot the uncertainty curb\n",
    "\n",
    "domain = 5\n",
    "x = np.linspace(-domain+0.5, domain+0.5, 100)\n",
    "y = np.linspace(-domain, domain, 100)\n",
    "\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "X_grid = np.column_stack([xx.flatten(), yy.flatten()])\n",
    "X = np.column_stack([xx.flatten(), yy.flatten()])\n",
    "\n",
    "X_vis, y_vis = sklearn.datasets.make_moons(n_samples=500, noise=noise)\n",
    "mask = y_vis.astype(np.bool)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output, confidence, embedding, embeddingDM, _ = model(torch.from_numpy(X_grid).float().cuda())\n",
    "    output = torch.max(output, dim=1)\n",
    "    confidence = 1 - nn.Sigmoid()(confidence)\n",
    "print('CHECKING confidence',confidence,'(min,max )=',confidence.min().item(),confidence.max().item())\n",
    "\n",
    "confidence = confidence.reshape(xx.shape).cpu()\n",
    "output = output[1].reshape(xx.shape).cpu()\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.contourf(x, y, confidence, cmap='cividis')\n",
    "plt.scatter(X_vis[mask, 0], X_vis[mask, 1])\n",
    "plt.scatter(X_vis[~mask, 0], X_vis[~mask, 1])\n",
    "plt.scatter(ood_x[:,0], ood_x[:,1])\n",
    "plt.title('uncertainty map')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.contourf(x, y, output, cmap='cividis')\n",
    "plt.scatter(X_vis[mask, 0], X_vis[mask, 1])\n",
    "plt.scatter(X_vis[~mask, 0], X_vis[~mask, 1])\n",
    "plt.scatter(ood_x[:,0], ood_x[:,1])\n",
    "plt.title('decision map')\n",
    "\n",
    "name = 'results_uncer_AFTERtraining_gaussian_ANDPCA.png'\n",
    "fig.savefig(name, dpi=200)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "188a29b1e720e88c396710f3d59a7f9b19cd91ce14785e635cef9c9e2a83e2ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
